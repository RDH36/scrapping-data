Skip to content
Showcase
Docs
Blog
Templates
Enterprise
Search documentation...
Feedback
Learn

Using App Router

Features available in /app

Using Stable Version

v15.0.2

Getting Started
Installation
Project Structure
Building Your Application
Routing
Defining Routes
Pages
Layouts and Templates
Linking and Navigating
Error Handling
Loading UI and Streaming
Redirecting
Route Groups
Project Organization
Dynamic Routes
Parallel Routes
Intercepting Routes
Route Handlers
Middleware
Internationalization
Data Fetching
Data Fetching and Caching
Server Actions and Mutations
Incremental Static Regeneration (ISR)
Rendering
Server Components
Client Components
Composition Patterns
Partial Prerendering
Runtimes
Caching
Styling
CSS
Tailwind CSS
Sass
CSS-in-JS
Optimizing
Images
Videos
Fonts
Metadata
Scripts
Package Bundling
Lazy Loading
Analytics
Instrumentation
OpenTelemetry
Static Assets
Third Party Libraries
Memory Usage
Configuring
TypeScript
ESLint
Environment Variables
Absolute Imports and Module Path Aliases
MDX
src Directory
Custom Server
Draft Mode
Content Security Policy
Debugging
Progressive Web Applications (PWA)
Testing
Vitest
Jest
Playwright
Cypress
Authentication
Deploying
Production Checklist
Static Exports
Multi-Zones
Upgrading
Codemods
Version 15
Version 14
App Router Migration
Migrating from Create React App
Migrating from Vite
Examples
API Reference
Directives
use cache
use client
use server
Components
Font
<Form>
<Image>
<Link>
<Script>
File Conventions
default.js
error.js
instrumentation.js
layout.js
loading.js
mdx-components.js
middleware.js
not-found.js
page.js
route.js
Route Segment Config
template.js
Metadata Files
favicon, icon, and apple-icon
manifest.json
opengraph-image and twitter-image
robots.txt
sitemap.xml
Functions
cacheTag
connection
cookies
draftMode
fetch
generateImageMetadata
generateMetadata
generateSitemaps
generateStaticParams
generateViewport
headers
ImageResponse
NextRequest
NextResponse
notFound
permanentRedirect
redirect
revalidatePath
revalidateTag
unstable_after
unstable_rethrow
useParams
usePathname
useReportWebVitals
useRouter
useSearchParams
useSelectedLayoutSegment
useSelectedLayoutSegments
userAgent
next.config.js Options
appDir
assetPrefix
basePath
cacheLife
compress
crossOrigin
cssChunking
devIndicators
distDir
dynamicIO
env
eslint
expireTime
exportPathMap
generateBuildId
generateEtags
headers
httpAgentOptions
images
cacheHandler
logging
mdxRs
onDemandEntries
optimizePackageImports
output
pageExtensions
poweredByHeader
ppr
productionBrowserSourceMaps
reactCompiler
reactMaxHeadersLength
reactStrictMode
redirects
rewrites
sassOptions
serverActions
serverComponentsHmrCache
serverExternalPackages
staleTimes
staticGeneration*
trailingSlash
transpilePackages
turbo
typedRoutes
typescript
urlImports
useLightningcss
webpack
webVitalsAttribution
CLI
create-next-app
next CLI
Edge Runtime
Legacy APIs
unstable_cache
unstable_noStore
Architecture
Accessibility
Fast Refresh
Next.js Compiler
Supported Browsers
Turbopack
Community
Contribution Guide
On this page
Static robots.txt
Generate a Robots file
Customizing specific user agents
Robots object
Version History
Managed Next.js (Vercel)
Scroll to top
File Conventions
Metadata Files
robots.txt
robots.txt

Add or generate a robots.txt file that matches the Robots Exclusion Standard
 in the root of app directory to tell search engine crawlers which URLs they can access on your site.

Static robots.txt
app/robots.txt
User-Agent: *
Allow: /
Disallow: /private/
Sitemap: https://acme.com/sitemap.xml
Generate a Robots file

Add a robots.js or robots.ts file that returns a Robots object.

Good to know: robots.js is a special Route Handlers that is cached by default unless it uses a Dynamic API or dynamic config option.

app/robots.ts
TypeScript
JavaScript
TypeScript
import type { MetadataRoute } from 'next'
 
export default function robots(): MetadataRoute.Robots {
  return {
    rules: {
      userAgent: '*',
      allow: '/',
      disallow: '/private/',
    },
    sitemap: 'https://acme.com/sitemap.xml',
  }
}

Output:

User-Agent: *
Allow: /
Disallow: /private/
Sitemap: https://acme.com/sitemap.xml
Customizing specific user agents

You can customise how individual search engine bots crawl your site by passing an array of user agents to the rules property. For example:

app/robots.ts
TypeScript
JavaScript
TypeScript
import type { MetadataRoute } from 'next'
 
export default function robots(): MetadataRoute.Robots {
  return {
    rules: [
      {
        userAgent: 'Googlebot',
        allow: ['/'],
        disallow: '/private/',
      },
      {
        userAgent: ['Applebot', 'Bingbot'],
        disallow: ['/'],
      },
    ],
    sitemap: 'https://acme.com/sitemap.xml',
  }
}

Output:

User-Agent: Googlebot
Allow: /
Disallow: /private/
User-Agent: Applebot
Disallow: /
User-Agent: Bingbot
Disallow: /
Sitemap: https://acme.com/sitemap.xml
Robots object
type Robots = {
  rules:
    | {
        userAgent?: string | string[]
        allow?: string | string[]
        disallow?: string | string[]
        crawlDelay?: number
      }
    | Array<{
        userAgent: string | string[]
        allow?: string | string[]
        disallow?: string | string[]
        crawlDelay?: number
      }>
  sitemap?: string | string[]
  host?: string
}
Version History
Version	Changes
v13.3.0	robots introduced.
Previous
opengraph-image and twitter-image
Next
sitemap.xml

Was this helpful?

supported.
Send
Resources
Docs
Learn
Showcase
Blog
Analytics
Next.js Conf
Previews
More
Next.js Commerce
Contact Sales
GitHub
Releases
Telemetry
Governance
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
Cookie Preferences
Subscribe to our newsletter

Stay updated on new releases and features, guides, and case studies.

Subscribe

Â© 2024 Vercel, Inc.